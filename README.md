# VodaAgileBoard

VodaAgileBoard is the tool for scrum masters and leaders: a Node.js web application for generating sprint reports from Jira for MPSA and MAS projects. It provides a preview-first workflow where you configure filters, preview data in a tabbed interface, and export CSV or Excel without re-fetching from Jira.

This README is the SSOT for usage and validation. Supplemental documents (e.g. `Jira-Reporting-Gap-Analysis-Plan.md`) provide planning context only and do not supersede this guide.

## Features

- **Preview-First Workflow**: Preview data before exporting to ensure accuracy
- **Sidebar Navigation**: Desktop left sidebar and mobile hamburger drawer for fast cross-page navigation
- **Multi-Project Support**: Generate reports for MPSA and MAS projects
- **Sprint Overlap Filtering**: Automatically filters sprints that overlap with the selected date window
- **Comprehensive Metrics**: Optional metrics including throughput, predictability, rework ratio, and Epic TTM
- **Flexible Export**: Export filtered or raw preview data as CSV
- **Runtime Discovery**: Automatically discovers boards and field IDs from your Jira instance
- **Error Handling**: Robust error handling with user-friendly messages and retry logic
- **Feedback Capture**: In-app feedback form for users to submit issues and suggestions
- **Project/Board SSOT**: Selected projects are shared across Report, Leadership, and Current Sprint via `vodaAgileBoard_selectedProjects` in localStorage. Report persists project checkboxes on change; Leadership reads and writes the same key; Current Sprint reads the same key but **normalizes to one project** for sprint-level accuracy and loads boards for that one project (fallback `MPSA`).
- **Filter Persistence**: Report search inputs (Boards/Sprints/Stories) persist between visits. Report and Leadership share the same date-range storage.
- **Current Sprint Transparency**: Squad view at `/current-sprint` - sprint header with name/ID, summary strip (work items, SP, % done) with a **stuck prompt** when any issue is in progress >24h (link to follow up), status chips, a **Project** selector synchronized from shared SSOT but enforced to one project in this page, single **sub-task summary** line in the summary card (logged h; missing estimate / no log; stuck >24h count) linking to the full Sub-task time tracking card, daily completion (with SP), burndown with ideal line + axis labels, scope changes (including reporter/assignee and merged risk rows), **Work items in sprint** table with **Type**, **Reporter**, **Assignee**, and merged subtask estimate/logged-hour columns, sub-task time tracking (estimate/logged/remaining plus status age), assignee or reporter notification message generator for missing sub-task time, dependencies/learnings, stuck tasks card (in progress >24h) with status-change hint, snapshot freshness badge (Live vs Snapshot timestamp), previous/next sprint snippet, and sprint tabs (latest to oldest by end date). Export menu now provides **Copy as Text**, **Markdown**, **Copy link**, and **Email**.
- **Persistent Notification Dock**: A fixed left-side alert dock appears across pages when time-tracking alerts exist. On Report and Leadership it stays compact and points users to **Open Current Sprint**; on Current Sprint it expands to show board/sprint details and missing estimate/log counts. It can be minimized or hidden after review, with a quick toggle to restore it.
- **Sprint Leadership View**: Normalized trends at `/sprint-leadership` - indexed delivery, predictability, no rankings. Quarter quick-pick shows year and period (e.g. "Q2 2025"); clicking a quarter loads data immediately. Remembers the last selected date range in the browser.

## Prerequisites

- Node.js 20.0.0 or newer
- Jira Cloud account with API access
- Jira API token (create at https://id.atlassian.com/manage-profile/security/api-tokens)

## Installation

1. Clone or download this repository

2. Install dependencies:
```bash
npm install
```

3. Copy `.env.example` to `.env`:
```bash
cp .env.example .env
```

4. Edit `.env` and set your Jira credentials:
```
JIRA_HOST=https://your-domain.atlassian.net
JIRA_EMAIL=your@email.com
JIRA_API_TOKEN=your_api_token
```

## Running the Application

### Development Mode
```bash
npm run dev
```

This starts the server with nodemon for auto-restart on file changes.

### Production Mode
```bash
npm start
```

This runs `npm run build:css` first (prestart), then starts the server. The server will start on `http://localhost:3000` (or the port specified in the `PORT` environment variable).

### Access the Application

1. Open your browser and go to `http://localhost:3000` (or the port in `PORT`).
2. Log in with the credentials configured in your environment (see Environment Variables).
3. After login you can open **Report**, **Current Sprint** (squad view), or **Sprint Leadership** from the app; the default redirect is `http://localhost:3000/report`.
4. Navigation is unified through a left sidebar on desktop and a hamburger drawer on mobile for **High-Level Performance**, **Current Sprint (Squad)**, and **Leadership HUD**.

### Quickstart for Scrum Masters & Leaders

1. Get the live VodaAgileBoard URL from your admin (for example, `https://voda-agile-board.onrender.com`).
2. Sign in with the credentials shared by your admin.
3. On the General Performance (Report) screen, keep both MPSA and MAS selected for a combined view, or choose a single project for a focused view.
4. Leave the default quarter dates or adjust them to your sprint window. Preview auto-runs after filter changes (manual **Preview** is still available).
5. Use **Export to Excel - All Data** to download a workbook you can slice and share in your own tooling.

## Usage

### Generating a Report

1. **Select Projects**: Check MPSA and/or MAS (at least one required)

2. **Set Date Window**: 
  - **Quick range (Vodacom quarters):** A scrollable strip of quarter pills (at least 5 quarters up to current) shows fiscal labels (e.g. "FY26 Q2"); clicking a pill sets the range and can auto-run the report. Select a quarter or enter dates manually.
   - Default is Q2 2025 (July 1 - September 30, 2025)
   - Adjust start and end dates as needed
   - Dates are in UTC

3. **Configure Options**:
   - **Story Points, Epic TTM, and Bugs/Rework**: Always included in reports (mandatory)
   - **Require Resolved by Sprint End** (optional): Only include stories resolved before sprint end
   - **Include Predictability** (optional): Calculate committed vs delivered (approx or strict mode)
   - **Include Active/Missing End Date Sprints** (optional): Include sprints with missing end dates

4. **Preview (auto or manual)**: Generates preview data from Jira.

5. **Review Tabs**:
  - **Project & Epic Level**: Shows discovered boards and all project/epic-level metrics in one consolidated view. Boards table merges delivery volume with time-normalized output (total sprint days, avg sprint length, **Done Stories**, **Registered Work Hours**, **Estimated Work Hours**, stories/SP per sprint day, variance, done-by-end %, epic vs non-epic counts), plus sprint window and latest sprint end. Includes a top-row **All Boards (Comparison)** summary to anchor comparisons. Includes capacity proxies (Active Assignees, Stories/SP per Assignee, Assumed Capacity, Assumed Waste %) with clear assumptions. Epic Time-To-Market shows Epic Name, story IDs as Jira links with hover summaries, **Subtask Spent (Hrs)** for the epic, and includes a **{Board}-AD-HOC** row per board for stories not linked to any epic. Missing epic titles are surfaced with a warning for trust. Throughput remains available by issue type, along with rework ratio and predictability. Includes per-section CSV export button.
   - **Sprints**: Lists sprints overlapping the date window with completion counts. Shows "Total SP" and "Story Count" columns. Column labels: "Stories Completed (Total)" (all stories currently marked Done) and "Completed Within Sprint End Date" (stories resolved by sprint end date). When time-tracking data exists, shows Est Hrs, Spent Hrs, Remaining Hrs, and Variance Hrs. When subtask tracking exists, adds Subtask Est/Spent/Remaining/Variance columns. Includes per-section CSV export button.
   - **Done Stories**: Drill-down view of completed stories, grouped by sprint. Shows Epic Key, Epic Title, and Epic Summary columns when Epic Link field is available. Epic Summary is truncated to 100 characters with full text in tooltip. When time tracking exists, shows Est/Spent/Remaining/Variance hours for the story and for its subtasks (when available). Dates render in local-friendly format with raw ISO on hover. Includes per-section CSV export button.
   - **Unusable Sprints**: Lists sprints excluded due to missing dates

6. **Export to Excel**:
   - **Export to Excel - All Data**: Main export button generates a comprehensive Excel workbook (.xlsx) with 6 tabs:
     - **Summary**: Key metrics, KPIs, agile maturity assessment, data quality scores, and manual enrichment guide
     - **Boards**: Board-level delivery and time-normalized metrics (sprint days, stories/SP per sprint day, variance, done-by-end %, epic vs non-epic counts, capacity proxy columns)
     - **Stories**: All done stories with business-friendly column names, Excel-compatible dates, calculated KPI columns (Work Days to Complete, Cycle Time, etc.), and manual enrichment columns (Epic ID/Name Manual, Is Rework/Bug Manual, Team Notes)
     - **Sprints**: Sprint-level metrics with throughput, predictability, rework data, and time tracking totals (when available)
   - **Epics**: Epic TTM data with calculated lead times, epic names, and linked story IDs
   - **Metadata**: Export timestamp, date range, projects, filters applied, and data freshness
   - **Field Inventory** (Metadata): Counts of available/custom Jira fields plus EBM-relevant field matches and missing candidates (no full field list in export payload)
   - **File Naming**: Excel files use descriptive names: `{Projects}_{DateRange}_{ReportType}_{ExportDate}.xlsx` (e.g., `MPSA-MAS_Q2-2025_Sprint-Report_2025-01-27.xlsx`)
   - **Business-Friendly Columns**: All technical column names are mapped to business-friendly labels (e.g., `issueKey` -> `Ticket ID`, `sprintStartDate` -> `Sprint Start Date`)
   - **Excel-Compatible Dates**: All dates are formatted for Excel recognition, enabling date filtering, pivot tables, and formulas
   - **KPI Columns**: Pre-calculated columns include Work Days to Complete, Cycle Time, Sprint Duration, and Agile Maturity Level
   - **Manual Enrichment**: Blank columns provided for teams to fill in missing Epic IDs/Names, Rework/Bug flags, and notes
   - **Data Validation**: Excel export validates data structure before sending to server, preventing errors and providing clear feedback
   - **Empty Tab Handling**: Empty tabs (Epics, Sprints) show placeholder messages explaining why data is missing
   - **File Size Warnings**: Large Excel files (>50MB) trigger a warning before generation, allowing users to filter data or cancel
   - **Improved Error Messages**: Specific, actionable error messages for network errors, server errors, timeouts, and invalid data
   
7. **Export CSV** (Secondary Option):
   - **Per-Section Exports**: Each tab has its own "Export CSV" button for quick single-section exports
   - **Export CSV (Filtered View)**: Exports only currently visible rows (after search/filter)
   - **File Naming**: `{Projects}_{DateRange}_{Section}_{ExportDate}.csv` (includes `_PARTIAL` when preview data is partial)
   - All CSV exports include Epic Key, Epic Title, and Epic Summary columns when Epic Link field is available
   - Stories exports include time-tracking and EBM-supporting fields when available (e.g., subtask count, story estimate/spent/remaining/variance hours, subtask estimate/spent/remaining/variance hours, status category, priority, labels, components, fix versions, and EBM fields such as team, product area, customer segments, value, impact, satisfaction, sentiment, severity, source, work category, goals, theme, roadmap, focus areas, delivery status/progress)

## Recent UX & Reliability fixes (2026-02-09)
- **Export visibility:** Export Excel and export dropdown are hidden until a preview has run successfully; they appear only when there is preview data to export.
- **Closest-available data banner:** When the server returns a subset cache (e.g. same projects, different date window), the UI shows "Showing closest available data for your selection. Use Full refresh for exact filters."
- **Loading hint:** Report loading panel shows "Usually ready in under 30s for one quarter." below the progress bar.
- **Partial-on-error cache:** If a preview request fails or times out, any data already retrieved is cached (when it has more rows than existing cache) so future or repeated requests benefit; partial entries use a shorter TTL (10 min) so full runs can replace them sooner.
- **Error UI hierarchy:** Error panel promotes "Use smaller date range" and "Re-run exact range"; "View technical details" is demoted to a secondary toggle.
- **Sticky chips row:** Report applied-filters chips row (and Edit filters) is sticky so filters are always reachable when scrolled.
- **One empty state:** Report uses a single empty-state message for no done stories with one "Adjust filters" CTA.
- **Generated X min ago:** Report sticky summary shows freshness (e.g. "Generated just now" or "Generated N min ago") when preview has meta.
- **Loading chip minimum 300 ms:** Report loading-status chip only appears after 300 ms to avoid flicker on fast previews.
- **Login copy:** One-line outcome ("Sprint risks and delivery in under 30 seconds"), shorter trust line, error focus, and rate-limit message (`?error=ratelimit`).
- **Leadership auto-preview:** Quarter or date change triggers preview without a mandatory Preview click.
- **Current Sprint copy:** Clearer loading text ("Choose projects above… Then pick a board") and no-boards error with hint ("Check project selection or try Report…").
- **Report filters tip and subtitle:** Shortened to one sentence; optional "Fast: pick a quarter" label.
- **Edge cases:** (1) Only latest preview result applied when filters change repeatedly; (2) No partial banner when 0 rows—unified empty state only; (3) Session-expiry message and redirect so user returns to Report after re-login.
- Report filters keep the last successful results visible while refreshing automatically.
- Leadership filters now auto-run preview on project/date changes and quarter picks.
- Report advanced options are now collapsed by default behind an explicit `Options` toggle.
- Long-range preview splitting now also activates for heavier project combinations.
- Current Sprint now renders one merged **Work risks** table combining scope changes, stuck items, sub-task tracking risks, and sprint ownership gaps.
- Notification dock now renders as a persistent left-side rail instead of covering right-side actions and defaults to a compact summary on Report/Leadership so it never competes with primary content.
- Playwright telemetry now ignores abort-class request failures caused by intentional cancellation.
- **Preview button state:** Single source of truth: project/date change and "Select none" call `refreshPreviewButtonLabel` (via `window.__reportPreviewButtonSync`) so disabled state and title stay in sync.
- **Tests:** Four Projects Q4 and Preview timeout specs force filters panel expanded (or use `force: true` for date fill) so date inputs are actionable; E2E "no projects" tests are skipped until flaky run is resolved.
- **Visual refresh without flow changes:** Existing pages now use a more modern, higher-contrast theme (lighter gradients, clearer hierarchy for sidebar/nav, stronger active states, and improved table/header readability) to reduce "plain" appearance while preserving the same workflows and controls.
- **Text encoding cleanup:** Fixed mojibake in core report/leadership rendering strings (for example timeline separators and grade fallback labels) to keep customer-facing copy trustworthy and readable.
- **Realtime fail-fast validation suite:** Added `tests/Jira-Reporting-App-Customer-Speed-Simplicity-Trust-Realtime-Validation-Tests.spec.js` and orchestration wiring so `npm run test:all` now includes 16 fail-fast checks covering report/current-sprint/leadership UI geometry, deduped throughput rendering, hydration behavior, and realtime telemetry ("logcat-equivalent") guardrails.

### Preview Behaviour & Feedback

- **In-flight feedback**:
  - When you click **Preview**, the button is temporarily disabled to prevent double-clicks while the loading overlay shows progress updates.
  - The loading panel includes a live timer and step log (e.g. Collecting filter parameters, Sending request to server, Received data from server).
  - A compact **loading status chip** appears near the bottom of the viewport while a preview is in-flight so you can scroll freely and still see that work is in progress.
  - If a previous preview is already visible, the UI keeps it on-screen while the new request runs and shows a refresh banner so users see immediate results.
  - The step log keeps only the most recent entries to avoid overwhelming the UI during long-running previews.
  - Partial previews include a **Force full refresh** action which re-runs the request with cache bypass.
- **User feedback capture**:
  - Click **Give Feedback** at the top of the report to submit your email and detailed feedback.
  - Submissions are stored on the server in `data/JiraReporting-Feedback-UserInput-Submission-Log.jsonl`.
- **Preview client-side timeout**
  - The report preview request has a client-side timeout (typically 60-90 seconds depending on date range and options). If the request exceeds this, the client aborts it.
  - On timeout, the error box shows a concise message: "Preview ran longer than Xs. We kept your last full results on-screen; try a smaller date range or fewer projects." with **Retry now**, **Retry with smaller date range**, and **Force full refresh** buttons. The error box is never left empty.
  - A dedicated Playwright spec (`Jira-Reporting-App-Preview-Timeout-Error-UI-Validation-Tests.spec.js`) validates that the error UI is visible, non-empty, and includes retry actions when a preview fails.
- **Partial previews**:
  - If the backend has to stop early (for example due to time budget or pagination limits), the response is marked as partial.
  - The UI shows a short warning banner near the preview summary and a matching hint near the export buttons, explaining that export matches exactly what is on-screen and recommending narrower ranges for full history. CSV exports will only contain the currently loaded data in this case.
- **Require Resolved by Sprint End**:
  - When this option is enabled, the **Done Stories** tab will explain when no rows passed this filter, and suggests turning it off or reviewing resolution vs sprint end dates in Jira.
- **Exports and table state**:
  - Export buttons are hidden until a preview has run successfully; they then appear and are enabled when there is data to export.
  - If you change filters and end up with no rows, the empty state explains whether this is due to filters, the Require Resolved by Sprint End option, or genuinely no Done stories.
  - Filtered CSV export is disabled when filters match zero rows, with a hint explaining why.
  - Invalid date inputs are caught client-side with a clear error before any request is sent.

### Filtering Done Stories

- **Search Box**: Filter by issue key or summary (case-insensitive)
- **Project Pills**: Click project pills to filter by project
- Filters update the visible rows in real-time

## API Endpoints

### GET /report
Serves the main report page HTML.

### GET /current-sprint
Serves the Current Sprint Transparency HTML page (squad view).

### GET /sprint-leadership
Serves the Sprint Leadership view HTML page.

### GET /api/boards.json
Returns a list of boards for the given projects (for the current-sprint board selector).

**Query Parameters:**
- `projects` (required): Comma-separated project keys (e.g., `MPSA,MAS`)

**Response:** `{ boards: Array<{ id: number, name: string, ... }> }`. Returns 400 with code `NO_PROJECTS` when `projects` is missing or empty.

### GET /api/current-sprint.json
Returns the current-sprint transparency payload for a board (snapshot-first; use `live=true` to bypass cache). Optional `sprintId` loads a specific sprint for tab navigation.

### POST /api/current-sprint-notes
Saves dependencies/learnings notes for a sprint. Body: `{ boardId, sprintId, dependencies, learnings }`.

**Query Parameters:**
- `boardId` (required): Jira board ID
- `projects` (optional): Comma-separated project keys
- `live` (optional): `true` to fetch live from Jira instead of cached snapshot

**Response:** Current-sprint payload (sprint details, daily completion, scope changes, burndown context). Returns 400 with code `MISSING_BOARD_ID` when `boardId` is missing; 404 with code `BOARD_NOT_FOUND` when the board is not in the given projects.

### GET /preview.json
Generates preview data from Jira.

**Query Parameters:**
- `projects` (required): Comma-separated project keys (e.g., `MPSA,MAS`)
- `start` (optional): Start date in ISO 8601 format (default: `2025-07-01T00:00:00.000Z`)
- `end` (optional): End date in ISO 8601 format (default: `2025-09-30T23:59:59.999Z`)
- `includeStoryPoints` (mandatory): Always `true` - Story Points are always included in reports
- `requireResolvedBySprintEnd` (optional): `true` or `false`
- `includeBugsForRework` (mandatory): Always `true` - Bugs/Rework are always included in reports
- `includePredictability` (optional): `true` or `false`
- `predictabilityMode` (optional): `approx` or `strict` (default: `approx`)
- `includeEpicTTM` (mandatory): Always `true` - Epic TTM is always included in reports
- `includeActiveOrMissingEndDateSprints` (optional): `true` or `false`
 - `previewMode` (optional): `normal` (default), `recent-first`, or `recent-only`. Heavy queries automatically prefer `recent-first`/`recent-only` to prioritise the last 14 days while leaning on cache for older history.
 - `preferCache` (optional): `true` to allow the server to return a best-available subset cache when the exact key misses (response meta includes `reducedScope` / `cacheMatchType`).
 - `clientBudgetMs` (optional): Soft time budget in milliseconds requested by the client; the server clamps this to an internal maximum and uses it as the preview time budget for partial responses.

### POST /export-excel
Generates Excel workbook (.xlsx) with multiple sheets.

**Request Body:**
```json
{
  "workbookData": {
    "sheets": [
      {
        "name": "Summary",
        "columns": ["Section", "Metric", "Value"],
        "rows": [...]
      },
      {
        "name": "Stories",
        "columns": ["Ticket ID", "Ticket Summary", ...],
        "rows": [...]
      }
    ]
  },
  "meta": {
    "selectedProjects": ["MPSA", "MAS"],
    "windowStart": "2025-07-01T00:00:00.000Z",
    "windowEnd": "2025-09-30T23:59:59.999Z"
  }
}
```

**Response:** Excel file download (.xlsx) with filename: `{Projects}_{DateRange}_{ReportType}_{ExportDate}.xlsx`

**Response:**
```json
{
  "meta": {
    "selectedProjects": ["MPSA", "MAS"],
    "windowStart": "2025-07-01T00:00:00.000Z",
    "windowEnd": "2025-09-30T23:59:59.999Z",
    "discoveredFields": {
      "storyPointsFieldId": "customfield_10016",
      "epicLinkFieldId": "customfield_10014"
    },
    "fromCache": false,
    "cacheAgeMinutes": 5,
    "epicTTMFallbackCount": 2
  },
  "boards": [...],
  "sprintsIncluded": [...],
  "sprintsUnusable": [...],
  "rows": [...],
  "metrics": {...}
}
```

### POST /export
Streams CSV export for large datasets.

**Request Body:**
```json
{
  "columns": ["projectKey", "boardId", ...],
  "rows": [{...}, {...}]
}
```

**Response:** CSV file download

## Testing

### Run All Tests
```bash
npm run test:all
```

This runs the test orchestration script which:
1. Installs dependencies
2. Runs API integration tests (includes `/api/boards.json`, `/api/current-sprint.json`, `/current-sprint`, `/sprint-leadership`)
3. Runs Login Security Deploy Validation tests
4. Runs E2E user journey tests
5. Runs UX reliability tests (validates data quality indicators, error handling, UI improvements)
6. Runs UX critical fixes tests (validates Epic Title/Summary, merged throughput, renamed labels, per-section exports, TTM definition, export loading states, button visibility)
7. Runs Feedback & Date Display tests
8. Runs Column Titles & Tooltips tests
9. Runs Validation Plan tests
10. Runs Excel Export tests
11. Runs Refactor SSOT Validation tests
12. Runs Boards Summary Filters Export Validation tests
13. Runs Current Sprint and Leadership View tests
14. Runs UX Trust and Export Validation tests (report, current-sprint, leadership, export; telemetry + UI)
15. Runs Current Sprint UX and SSOT Validation tests (board pre-select, burndown summary, empty states, leadership empty preview)
16. Terminates on first error
17. Shows all steps in foreground with live output from each test command and elapsed step timing. Step definitions live in `scripts/Jira-Reporting-App-Test-Orchestration-Steps.js`.

### Run Specific Test Suites
```bash
# E2E tests only
npm run test:e2e

# API tests only
npm run test:api

# Validation plan tests (UI + telemetry)
npm run test:validation

# Current Sprint and Leadership view E2E tests
npm run test:current-sprint-leadership

# UX Trust Validation (report, current-sprint, leadership + console/UI assertions)
npm run test:ux-trust

# Current Sprint UX and SSOT Validation (board pre-select, burndown, empty states, leadership empty preview)
npm run test:current-sprint-ux-ssot
```

### Test Coverage and Caching Behavior

- **E2E Tests**: User interface interactions, tab navigation, filtering, export
- **API Tests**: Endpoint validation, error handling, CSV generation
- **UX Reliability Tests**: Data quality indicators (Unknown issueType display, Epic TTM fallback warnings), cache age display, error recovery
- **UX Critical Fixes Tests**: Epic Title/Summary display, merged Sprint Throughput data, renamed column labels with tooltips, per-section CSV export buttons and filenames, TTM definition header, export button loading states, button visibility after async renders, Epic Summary truncation edge cases
- **Excel Export Tests**: Excel file generation, multi-tab structure, business-friendly column names, Excel-compatible dates, KPI calculations, manual enrichment columns, Summary and Metadata tabs

**Note**: Some tests may require valid Jira credentials. Tests that require Jira access will gracefully handle authentication failures.

### Data Quality & Reliability Features

- **Issue Type Tracking**: All rows include `issueType` field. Missing types display as "Unknown" in UI and are logged as warnings.
- **Epic Data Enrichment**: When Epic Link field is available, rows include Epic Key, Epic Title, and Epic Summary. Epic Summary is truncated to 100 characters in table view with full text in tooltip. Epic fetch failures gracefully degrade - empty strings are used if Epic issues unavailable.
- **Epic TTM Accuracy**: Epic TTM uses Epic issue dates when available. Falls back to story dates if Epic issues unavailable, with warning displayed in Metrics tab. Definition clearly explained: "Days from Epic creation to Epic resolution (or first story created to last story resolved if Epic dates unavailable)."
- **Cache Transparency**: Preview meta shows cache age when data is served from cache, enabling users to assess data freshness.
- **Error Recovery**: Epic fetch failures don't break preview generation - system gracefully degrades to story-based calculation.
- **Excel Export**: Main export generates comprehensive Excel workbook with 5 tabs (Summary, Stories, Sprints, Epics, Metadata). Files use descriptive naming: `{Projects}_{DateRange}_{ReportType}_{ExportDate}.xlsx`. All dates are Excel-compatible format, enabling filtering and formulas. Data is validated before export, empty tabs show placeholder messages, large files (>50MB) trigger warnings, and error messages are actionable.
- **Business-Friendly Columns**: Technical column names mapped to business-friendly labels (e.g., `issueKey` -> `Ticket ID`, `sprintStartDate` -> `Sprint Start Date`) for easier analysis by leaders and analysts.
- **KPI Columns**: Pre-calculated columns include Work Days to Complete, Cycle Time (Days), Sprint Duration (Work Days), Days Since Created, and Agile Maturity Level (1-5 scale).
- **Manual Enrichment**: Excel exports include blank columns for teams to fill in: Epic ID (Manual), Epic Name (Manual), Is Rework (Manual), Is Bug (Manual), and Team Notes.
- **CSV Validation**: Client-side validation ensures required columns (issueKey, issueType, issueStatus) are present before export. CSV exports include Epic Key, Epic Title, and Epic Summary when available.
- **Export UX**: Export buttons show loading state ("Exporting..." or "Generating Excel...") and are disabled during export to prevent duplicate exports. Buttons are visible after async rendering completes.
- **Excel export (Report page)**: Export to Excel is available on the General Performance (Report) page after preview has data. It is validated by `Jira-Reporting-App-Excel-Export-Tests.spec.js` and by the UX Trust And Export Validation tests.
- **Current Sprint Work risks**: The Work risks table (Scope, Flow, Subtask, Sprint) shows issue summary and status from Jira when the API provides them; scope-change rows include summary and status from the server.

### Test Orchestration & Playwright

- The test orchestration script (`npm run test:all`) runs `npm install`, then (when the server is up) calls `POST /api/test/clear-cache` so no test reads stale in-memory cache. The clear-cache endpoint is available only when `NODE_ENV=test` or `ALLOW_TEST_CACHE_CLEAR=1`. The ordered list of steps is in `scripts/Jira-Reporting-App-Test-Orchestration-Steps.js`. It runs a sequence of Playwright specs (API integration, Server Errors and Export Validation, Login Security Deploy, E2E user journey, UX Reliability, UX Critical Fixes, UX Customer Simplicity Trust Full, Feedback, Column Tooltips, Validation Plan, Excel Export, Refactor SSOT, Boards Summary Filters Export, Current Sprint and Leadership View, UX Trust Validation, Current Sprint UX and SSOT Validation, Linkification and Empty-state UI Validation, Server Feedback Endpoint, Growth Velocity, and others) with `--headed`, `--max-failures=1`, and `--workers=1` (fail-fast on first failure). Steps include CSS Build And Mobile Responsive (viewport containment, headers, nav/filters). Spec files in `tests/` follow the naming convention `Jira-Reporting-App-*-Validation-Tests.spec.js` (or similar); obsolete files may be prefixed with `DeleteThisFile_`.
- Specs in `tests/` use `captureBrowserTelemetry(page)` (console errors, page errors, failed requests) and UI assertions so a step fails if the UI is wrong or the browser reports errors.
- **Issue key linkification:** Report Done Stories and Epic TTM use Jira links for issue keys; Current Sprint (Stories, Scope changes, Items stuck, Sub-task tracking) uses shared `renderIssueKeyLink(issueKey, issueUrl)` from `Reporting-App-Shared-Dom-Escape-Helpers.js`. Backend sends `issueKey` and `issueUrl`; optional `meta.jiraHost` in current-sprint response allows client fallback when URL is missing.
- **Empty-state SSOT:** `Reporting-App-Shared-Empty-State-Helpers.js` exports `renderEmptyStateHtml(title, message, hint)`; Report, Current Sprint, and Leadership use it for consistent "no data" messaging.
- Playwright is configured (via `playwright.config.js`) to:
  - Use `http://localhost:3000` as the default `baseURL` (configurable via `BASE_URL` for remote runs).
  - Optionally manage the application lifecycle with `webServer` (set `SKIP_WEBSERVER=true` to run against an already running server, e.g. when `BASE_URL` points to a deployed instance).

The backend maintains an in-memory TTL cache for several concerns:

- **Boards and Sprints**: Cached per project/board for 20 minutes to avoid redundant Jira calls.
- **Fields**: Story Points and Epic Link field IDs cached for 30 minutes.
- **Preview Responses**: Full `/preview.json` payloads cached for 20 minutes per unique combination of:
  - Sorted project list
  - Start/end window
  - All boolean toggles
  - `predictabilityMode`
- **Preview Partial (on error/timeout)**: When a request fails or times out, any accumulated rows are cached only if they improve on existing data (more rows); these entries use a 10-minute TTL so a later full run can replace them.

Cached preview responses are immutable snapshots. If Jira data changes within the TTL, those changes will not be reflected until the cache entry expires or the server restarts. This keeps repeated previews (with identical filters) fast and predictable.

## Project Structure

```
.
|-- server.js                 # Express server and routes
|-- package.json              # Dependencies and scripts
|-- .env.example              # Environment variable template
|-- .gitignore                # Git ignore rules
|-- lib/
|   |-- jiraClients.js        # Jira client setup
|   |-- cache.js              # TTL cache implementation
|   |-- discovery.js          # Board and field discovery
|   |-- sprints.js            # Sprint fetching and filtering
|   |-- currentSprint.js      # Current-sprint payload (imports Notes-IO, IssueType, Burndown-Resolve)
|   |-- issues.js             # Issue fetching, buildDrillDownRow re-export (imports Pagination-Fields, DrillDown-Row, Subtask-Time-Totals)
|   |-- metrics.js            # Metrics calculations
|   |-- csv.js                # CSV generation utilities
|   |-- excel.js              # Excel generation utilities
|   |-- columnMapping.js      # Business-friendly column name mapping
|   |-- kpiCalculations.js   # KPI calculation functions
|   |-- Jira-Reporting-App-Data-CurrentSprint-Notes-IO.js
|   |-- Jira-Reporting-App-Data-IssueType-Classification.js
|   |-- Jira-Reporting-App-Data-CurrentSprint-Burndown-Resolve.js
|   |-- Jira-Reporting-App-Data-Issues-Pagination-Fields.js
|   |-- Jira-Reporting-App-Data-Issues-DrillDown-Row.js
|   |-- Jira-Reporting-App-Data-Issues-Subtask-Time-Totals.js
|   `-- Jira-Reporting-App-Server-Logging-Utility.js  # Structured logging
|-- public/
|   |-- report.html           # General Performance report UI (modular entrypoint)
|   |-- Reporting-App-Report-Page-Init-Controller.js  # Report page init/controller (SSOT)
|   |-- Reporting-App-Report-Page-*.js               # Report page modules (state, filters, preview, renderers, exports)
|   |-- Reporting-App-Report-Page-Preview-Complexity-Config.js  # Preview complexity and timeout config
|   |-- Reporting-App-Shared-*.js                     # Shared helpers (DOM escape, formatting, boards summary, notifications, quarters)
|   |-- css/                  # CSS source partials (01-reset-vars through 08-modals-misc); run `npm run build:css` to output styles.css
|   `-- styles.css            # Built stylesheet (do not edit; generated from public/css/). Viewport containment (no horizontal overflow) and mobile responsiveness are validated by Mobile Responsive UX and CSS Build And Mobile Responsive validation specs.
|-- tests/
|   |-- JiraReporting-Tests-Shared-PreviewExport-Helpers.js  # SSOT for runDefaultPreview, waitForPreview, captureBrowserTelemetry
|   |-- Jira-Reporting-App-E2E-User-Journey-Tests.spec.js
|   |-- Jira-Reporting-App-API-Integration-Tests.spec.js
|   |-- Jira-Reporting-App-UX-Trust-And-Export-Validation-Tests.spec.js  # SSOT for report/current-sprint/leadership/export
|   |-- Jira-Reporting-App-UX-Reliability-Fixes-Tests.spec.js
|   |-- Jira-Reporting-App-UX-Critical-Fixes-Tests.spec.js
|   |-- Jira-Reporting-App-Excel-Export-Tests.spec.js
|   |-- Jira-Reporting-App-RED-LINE-ITEMS-KPI-Tests.spec.js
|   |-- Jira-Reporting-App-Current-Sprint-Leadership-View-Tests.spec.js
|   `-- (other .spec.js files)
`-- scripts/
    |-- Jira-Reporting-App-Test-Orchestration-Runner.js  # Runs steps; clear-cache, server start optional
    `-- Jira-Reporting-App-Test-Orchestration-Steps.js   # Step definitions (getSteps(projectRoot))
```

## Reality-check and UX backlog (Customer, Simplicity, Trust)

**Outcome lens:** People don’t buy products, they buy outcomes. First load today: user lands on /report and often sees an empty main area until they run Preview. That’s a weak first impression.

**Prioritized improvements (zero budget, incremental):**

1. **Report first-paint context (done)** – On load, the main area now shows a single outcome line (`#report-context-line`): last run summary and freshness when available, or “No report run yet.” So returning users see “Last: X stories, Y sprints · Generated N min ago” before doing anything.
2. **Dashboard from cache** – Use existing preview cache: on /report load, if server has cached preview for default projects/quarter, return a lightweight summary (e.g. from `lib/cache.js`) so the client can show “Last quarter: X boards, Y stories” without a full preview run. Option: GET /api/report-summary-from-cache that returns `{ boards, storyCount, generatedAt }` from last cached payload for current project/date key.
3. **One-click “Load latest”** – Replace or supplement the generic empty state with a single CTA: “See last quarter’s delivery — Load latest” that runs preview with default window so the user gets data in one click.
4. **Auto-preview on load when last-run exists** – When report loads with default projects and sessionStorage has last-run, trigger one auto-preview after a short delay (e.g. 1s) so returning users see data quickly without clicking.
5. **Background prefetch (later)** – Scheduled job or low-priority worker that warms cache for squads not yet searched, so next visit the landing can show something useful for every squad. No change to current flows; add only when resource allows.
6. **Login outcome line** – Keep and emphasize the existing “Sprint risks and delivery in under 30 seconds” (and trust line) on the login page so the outcome is clear before sign-in.

**Codebase health (Simplicity, SSOT):** Duplicate “skip if redirected to login” logic across many specs is consolidated into `skipIfRedirectedToLogin(page, test, options)` in `JiraReporting-Tests-Shared-PreviewExport-Helpers.js`. Further: merge duplicate logic in large files (e.g. `Reporting-App-Report-Page-Preview-Flow.js` is SIZE-EXEMPT; `lib/currentSprint.js` >300 lines — split by logical blocks when touching). Enforce ≤300 lines per file and single source of truth for routes, components, and tests; no parallel implementations.

## Metric guide and governance

Use metrics with explicit assumptions. Every view should make clear what is measured, what is assumed, and what could be wrong. Do not use metrics for performance review, ranking teams, or weaponizing numbers.

### Per-metric guardrails

- **Throughput (SP / stories per sprint)**  
  **Measures:** Volume of work completed in the window (story points and story count).  
  **Does not measure:** Quality, complexity, or team capacity.  
  **Can mislead when:** Sprint length or scope varies; SP is inconsistent across teams.  
  **Do not use for:** Comparing raw totals across teams; performance appraisal.

- **Predictability % (committed vs delivered)**  
  **Measures:** How much of the committed scope (at sprint start) was delivered by sprint end.  
  **Does not measure:** Whether scope change was justified or whether the team failed.  
  **Can mislead when:** Committed is approximated from creation date; late scope add is treated as failure.  
  **Do not use for:** Single-sprint team quality score; blaming teams for unplanned spillover.

- **Planned carryover vs unplanned spillover**  
  **Measures:** Delivered work that was in plan at sprint start vs added mid-sprint.  
  **Does not measure:** Why scope changed or whether it was appropriate.  
  **Do not use for:** Treating unplanned spillover as failure; ranking without context.

- **Rework % (bug SP vs story SP)**  
  **Measures:** Proportion of delivered effort that was bugs vs stories.  
  **Does not measure:** Root cause or whether bugs were regression vs new work.  
  **Can mislead when:** Bug definition or SP usage differs across teams.  
  **Do not use for:** Naming worst team; performance review.

- **Epic TTM (time to market)**  
  **Measures:** Calendar or working days from Epic start to Epic (or story) completion.  
  **Does not measure:** Value delivered or quality of the epic.  
  **Can mislead when:** Epic hygiene is poor (many stories without epic; epics spanning many sprints). Epic TTM is suppressed when hygiene is insufficient.  
  **Do not use for:** Comparing teams without normalizing for epic size or type.

- **Indexed delivery score**  
  **Measures:** Current SP per sprint day vs that teams own rolling average (last 3-6 sprints).  
  **Does not measure:** Absolute productivity or cross-team comparison.  
  **Can mislead when:** Used to rank teams; baseline period is unrepresentative.  
  **Do not use for:** Ranking teams; performance review.

- **Burndown / remaining SP by day**  
  **Measures:** Context for how remaining scope decreased over the sprint (when completion anchor is resolution date).  
  **Does not measure:** Effort or ideal line accuracy.  
  **Can mislead when:** Scope changes are not shown; used as a single success criterion.  
  **Do not use for:** Grading the sprint; ignoring scope-change context.

- **Daily completion histogram**  
  **Measures:** Stories (and optionally task movement) completed per calendar day.  
  **Does not measure:** Effort or quality.  
  **Do not use for:** Inferring slow days without scope/blocker context.

- **Observed work window**  
  **Measures:** Earliest and latest issue activity (created/resolution) in the sprint.  
  **Does not measure:** Whether sprint dates were wrong; only that work fell inside or outside planned dates.  
  **Do not use for:** Blaming teams when work extends past sprint end; use for transparency only.

### Metrics that look good but are not trustworthy

- **Raw SP totals across teams** - Sprint length and scope differ; normalize by sprint days and use indexed delivery for trend, not rank.
- **Sprint count as productivity** - More sprints do not mean more delivery; use stories/SP per sprint day.
- **Single-sprint predictability as team quality** - One sprint is noise; use planned vs unplanned breakdown and trends.
- **Unplanned spillover as failure** - Mid-sprint adds (bugs, support) are reality; show cause (Bug/Support/Feature), not blame.

### Example: misuse vs correct interpretation

- **Misuse:** Team A has lower predictability % than Team B, so Team A is underperforming.  
- **Correct:** Team A had more unplanned spillover (bugs/support). Check scope-change cause and sprint hygiene before comparing predictability.

## Troubleshooting

### Port already in use (EADDRINUSE)
- If you see "Port already in use" when starting the server, another process is bound to the port (e.g. a previous server instance).
- **Fix:** Stop the other process (e.g. close the terminal running `node server.js`) or set a different port: `PORT=3001 npm start`.
- Do not start a second server on the same port; the test orchestration reuses an existing server when the port is in use.

### "Missing required Jira credentials" Error
- Ensure `.env` file exists and contains `JIRA_HOST`, `JIRA_EMAIL`, and `JIRA_API_TOKEN`
- Verify the API token is valid and not expired
- Check that the email matches your Jira account

### "Failed to fetch boards" Error
- Verify project keys (MPSA, MAS) are correct
- Ensure your Jira account has access to these projects
- Check that the projects have boards configured
- Verify your `.env` file is in the project root (not in a subdirectory)
- Check server startup logs for credential loading confirmation

### "Rate limited" Error
- The app automatically retries with exponential backoff
- Wait a moment and try again
- Consider reducing the date range to fetch less data

### No Data in Preview
- Verify sprints exist in the selected date range
- Check that stories are marked as "Done" in Jira
- Ensure stories belong to the selected projects
- Try enabling "Include Active/Missing End Date Sprints" if sprints are missing end dates
- Check server logs for detailed error messages
- Large date ranges or many sprints may return a partial preview after ~1 minute to keep the UI responsive
- Quarterly ranges now use cached older sprints plus the most recent 2 weeks live to avoid timeouts; a full refresh can be forced with cache bypass

### Date Timezone Issues
- All dates are handled in UTC
- The UI shows both UTC and local time for reference
- Ensure your date inputs are correct for your timezone

## Environment Modes

VodaAgileBoard behaves slightly differently depending on which environment variables you set:

- **Local development (no auth, default)**:
  - Set Jira variables and (optionally) `PORT`, but **do not** set `SESSION_SECRET` or any `APP_LOGIN_*` values.
  - Visit `http://localhost:3000/report` directly; `/` redirects to `/report` for a fast feedback loop.
- **Local development (auth enabled)**:
  - In addition to Jira variables, set `SESSION_SECRET`, `APP_LOGIN_USER`, and `APP_LOGIN_PASSWORD`.
  - Visit `http://localhost:3000`; you will see the login screen, and `/report` plus the APIs require a valid session.
- **CI (GitHub Actions)**:
  - CI runs `npm run test:all` with a controlled set of environment variables.
  - Recommended: keep auth disabled in CI by omitting `SESSION_SECRET`, so most tests remain simple and deterministic.
- **Production (Render)**:
  - Always set `SESSION_SECRET`, `APP_LOGIN_USER`, `APP_LOGIN_PASSWORD`, Jira variables, and `NODE_ENV=production` from the Render dashboard.
  - End users always experience the login page plus honeypot, rate limiting, and session timeout before they reach `/report`.

## Environment Variables

- `JIRA_HOST`: Your Jira instance URL (e.g., `https://your-domain.atlassian.net`)
- `JIRA_EMAIL`: Your Jira account email
- `JIRA_API_TOKEN`: Your Jira API token
- `APP_LOGIN_USER`: Username for app login (required when auth is enabled)
- `APP_LOGIN_PASSWORD`: Password for app login (required when auth is enabled)
- `SESSION_SECRET`: Secret for signing session cookies (required when auth is enabled)
- `PORT`: Server port (default: 3000)
- `LOG_LEVEL`: Logging level - `DEBUG`, `INFO`, `WARN`, `ERROR` (default: `INFO`)
- `NODE_ENV`: Environment - `development` or `production`

## Deployment

VodaAgileBoard can be deployed to [Render](https://render.com) or any Node host.

1. Connect your Git repo (e.g. GitHub) to Render and create a Web Service.
2. Set **Build command** to `npm install` (or `npm ci`) and **Start command** to `npm start`.
3. Add all environment variables in the Render dashboard: `JIRA_HOST`, `JIRA_EMAIL`, `JIRA_API_TOKEN`, `APP_LOGIN_USER`, `APP_LOGIN_PASSWORD`, `SESSION_SECRET`, `NODE_ENV=production`.
4. Optional: use a [Blueprint](https://render.com/docs/infrastructure-as-code) by adding `render.yaml` at the repo root; validate with `render blueprints validate`. Deploy via Render CLI: `render deploys create <SERVICE_ID> --confirm`.

### Live instance

After the first deploy succeeds, your app will be available at a URL like `https://voda-agile-board.onrender.com`. Update this README with your actual live URL.

### CI/CD

Tests run on push via GitHub Actions (when configured). Deploys are triggered by Render on push to `main` (Git-backed), or by running the Render CLI in CI with `RENDER_API_KEY` and `RENDER_SERVICE_ID`.

## License

MIT

## Support

For issues or questions, please check the troubleshooting section above or review the error messages in the application UI.



